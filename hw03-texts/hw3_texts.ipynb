{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHgmxWG_7lnE"
   },
   "source": [
    "# Введение в анализ данных\n",
    "## НИУ ВШЭ, 2019-2020 учебный год\n",
    "\n",
    "### Домашнее задание №3\n",
    "\n",
    "Задание выполнил(а): (впишите свои фамилию и имя)\n",
    "\n",
    "### Общая информация\n",
    "\n",
    "__Дата выдачи:__ 06.04.2020\n",
    "\n",
    "__Дедлайн:__ 23:59 20.04.2020\n",
    "\n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Оценка за ДЗ вычисляется по следующей формуле:\n",
    "\n",
    "$$\n",
    "min(\\text{points}, 18)  \\times 10 / 18,\n",
    "$$\n",
    "\n",
    "где points — количество баллов за обязательную часть, которое вы набрали. Максимальное число баллов, которое можно получить за обязательную часть — 18, за каждые полтора балла сверху вы получите 1 бонусный балл (максимум 2). Также вы можете использовать бонусные баллы, которые накопили ранее.\n",
    "\n",
    "За сдачу задания позже срока на итоговую оценку за задание накладывается штраф в размере 1 балл в день, но получить отрицательную оценку нельзя.\n",
    "\n",
    "__Внимание!__ Домашнее задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов.\n",
    "\n",
    "### Формат сдачи\n",
    "\n",
    "Загрузка файлов с решениями происходит в системе [Anytask](https://anytask.org/).\n",
    "\n",
    "Инвайт для группы ИАД-6: rd5CNrr\n",
    "\n",
    "Перед отправкой перезагрузите ноутбук и проверьте, что все ячейки могут быть последовательно выполнены. Ноутбук должен запускаться с использованием python 3.6+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztx03xvr9T95"
   },
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BVrrwTJNjuDt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import *\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "colab_type": "code",
    "id": "_VMchexbjjTh",
    "outputId": "c1f66a1f-8851-42f3-e7e5-6c48d3c24707"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "# Качаем датасет\n",
    "\n",
    "!wget https://www.dropbox.com/s/tg55q9mrziroyrs/train_subset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rvXKae8q9nn-"
   },
   "source": [
    "### Данные\n",
    "\n",
    "Мы имеем дело с данными с торговой платформы Avito.\n",
    "Для каждого товара представлены следующие параметры:\n",
    " - title\n",
    " - description\n",
    " - Category_name\n",
    " - Category\n",
    "\n",
    "Имеется информация об объектах 50 классов.\n",
    "Задача: по новым объектам (title, description) предсказать Category.\n",
    "(Очевидно, что параметр Category_name для предсказания классов использовать нельзя)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "BqEuoDhqNgoa",
    "outputId": "b345f049-ae77-4d1b-a25f-4d4f447e63d2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>Category_name</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>382220</td>\n",
       "      <td>Прихожая</td>\n",
       "      <td>В хорошем состоянии. Торг</td>\n",
       "      <td>Мебель и интерьер</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>397529</td>\n",
       "      <td>Кордиант 215/55/16 Летние</td>\n",
       "      <td>Кордиант 215/55/16 Летние/\\n /\\nАртикул: 1737l...</td>\n",
       "      <td>Запчасти и аксессуары</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>584569</td>\n",
       "      <td>Стол</td>\n",
       "      <td>Стол, 2 рабочих места . Стол серого цвета, в д...</td>\n",
       "      <td>Мебель и интерьер</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2513100</td>\n",
       "      <td>Комбинезон</td>\n",
       "      <td>Размер-42/44</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1091886</td>\n",
       "      <td>Ветровка</td>\n",
       "      <td>На 2 года</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title  \\\n",
       "id                                   \n",
       "382220                    Прихожая   \n",
       "397529   Кордиант 215/55/16 Летние   \n",
       "584569                        Стол   \n",
       "2513100                 Комбинезон   \n",
       "1091886                   Ветровка   \n",
       "\n",
       "                                               description  \\\n",
       "id                                                           \n",
       "382220                           В хорошем состоянии. Торг   \n",
       "397529   Кордиант 215/55/16 Летние/\\n /\\nАртикул: 1737l...   \n",
       "584569   Стол, 2 рабочих места . Стол серого цвета, в д...   \n",
       "2513100                                       Размер-42/44   \n",
       "1091886                                          На 2 года   \n",
       "\n",
       "                     Category_name  Category  \n",
       "id                                            \n",
       "382220           Мебель и интерьер        20  \n",
       "397529       Запчасти и аксессуары        10  \n",
       "584569           Мебель и интерьер        20  \n",
       "2513100  Одежда, обувь, аксессуары        27  \n",
       "1091886     Детская одежда и обувь        29  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train_subset.csv\", index_col='id')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Kg8iPp7fiwGh",
    "outputId": "96ed00ed-b63b-4478-f2d4-66bda1110b5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1hvzAMETU2d"
   },
   "outputs": [],
   "source": [
    "X = data[['title', 'description']].to_numpy()\n",
    "y = data['Category'].to_numpy()\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tMYU7zZw_cw-"
   },
   "source": [
    "Сразу разделим выборку на train и test.\n",
    "Никакие данные из test для обучения использовать нельзя!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fia4_3vNprp"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "qDR8LtTJUIGt",
    "outputId": "fd4d5b55-a023-4129-9ff5-a6e8e24db915"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Сапоги 46 размер новые', 'Сапоги 46 размер новые'],\n",
       "       ['Светильники потолочный swarovski',\n",
       "        'светильники потолочные swarovski 6 штук , цена за штуку. В эксплуатации 2 года , продаются в связи со сменой интерьера в квартире'],\n",
       "       ['iPhone 7 plus 128GB Red красный в наличии',\n",
       "        '\\xa0/\\n/\\n Данная цена только для подписчиков Instagram: iQmac/\\n/\\n Новый красный айфон 7 Plus в наличии это элегантный и мощный смартфон, который готов в полной мере раскрыть новые возможности iOS 10. Аппарат с 4-ядерным процессором А10 и 3 ГБ ОЗУ с легкостью решает самые ресурсоемкие задачи, позволяя наслаждаться быстродействием «тяжелых» приложений и игр на 5,5-дюймовом дисплее. Аппарат получил экран, как у iPad Pro, так что картинка теперь соответствует кинематографическому стандарту.'],\n",
       "       ['Пион Ирис Ромашка рассада',\n",
       "        'Пион куст 500 р ( более 10 шт)/\\nСаженец/ корень 100р/\\nРастут у нас более 70 лет/\\nРозовые, бордовые и белые/\\nНа фото цветы 2018г/\\nП. Зубчаниновка/\\nлибо пл. Революции/\\nЕсть ирисы, ромашка, клубника, боярышник и ирга'],\n",
       "       ['Кофта', 'Состояние отличное']], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 27,  20,  84, 106,  27], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-ZEdlEGAXTD"
   },
   "source": [
    "### Токенизация (1 балл)\n",
    "\n",
    "\n",
    "Токенизация -- разбиение текста на мелкие части, которые можно обработать машинными методами.\n",
    "Можно использовать разные алгоритмы токенизации.\n",
    "Можете использовать WordPunctTokenizer или подобрать какой-то другой, если считаете, что он лучше подойдет для этой задачи.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: Здраствуйте. Я, Кирилл. Хотел бы чтобы вы сделали игру, 3Д-экшон суть такова...\n",
      "after: ['здраствуйте', '.', 'я', ',', 'кирилл', '.', 'хотел', 'бы', 'чтобы', 'вы', 'сделали', 'игру', ',', '3д', '-', 'экшон', 'суть', 'такова', '...']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "text = 'Здраствуйте. Я, Кирилл. Хотел бы чтобы вы сделали игру, 3Д-экшон суть такова...'\n",
    "\n",
    "print(\"before:\", text,)\n",
    "print(\"after:\", tokenizer.tokenize(text.lower()),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_RYBKC26o1X"
   },
   "source": [
    "__Задание:__ реализуйте функцию ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "O9VgNlZ1Qy3o",
    "outputId": "59ef3a75-008e-47c5-fba8-a319eba13ef4"
   },
   "outputs": [],
   "source": [
    "def preprocess(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Данная функция принимает на вход текст, \n",
    "    а возвращает тот же текст, но с пробелами между каждым токеном\n",
    "    \"\"\"\n",
    "    text_tokenized = re.findall(r'\\w+|[^\\w\\s]+', text)\n",
    "    text_tokenized = ' '.join(text_tokenized).lower()\n",
    "    return text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert preprocess(text) == 'здраствуйте . я , кирилл . хотел бы чтобы вы сделали игру , 3д - экшон суть такова ...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ токенизируйте title и description в train и test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z5WO-7tJUvbs"
   },
   "outputs": [],
   "source": [
    "X_train = np.apply_along_axis(np.vectorize(preprocess, otypes=['object']), 1, X_train)\n",
    "X_test = np.apply_along_axis(np.vectorize(preprocess, otypes=['object']), 1, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDnDSWwFDwFo"
   },
   "outputs": [],
   "source": [
    "assert X_train[10][1] == 'продам иж планета 3 , 76 год , ( стоит на старом учёте , документы утеряны ) на ходу , хорошее состояние , все интересующие вопросы по телефону ( с родной коляской на 3 тысячи дороже ) . торга не будет .'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hlIITUk0AsmS"
   },
   "source": [
    "### BOW (3 балла)\n",
    "\n",
    "Один из традиционных подходов -- построение bag of words.\n",
    "\n",
    "Метод состоит в следующем:\n",
    "\n",
    " - Составить словарь самых часто встречающихся слов в train data\n",
    " - Для каждого примера из train посчитать, сколько раз каждое слово из словаря в нём встречается\n",
    "\n",
    "\n",
    " В sklearn есть CountVectorizer, но в этом задании его использовать нельзя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GMKUttDWIF92"
   },
   "source": [
    "__Задание:__ создайте словарь, где в соответствии каждому токену стоит количество раз, которое оно встретилось в X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in X_train:\n",
    "    for j in i:\n",
    "        tokens.extend(j.split())\n",
    "tokens = dict(Counter(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokens['сапоги'] == 454"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ выведите 10 самых частотных и 10 самых редких токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 самых частотных токенов: [('/', 85802), (',', 79117), ('.', 65624), ('-', 36840), ('в', 28337), ('и', 21714), ('на', 19465), ('./', 17639), (':', 15098), ('с', 12860)]\n",
      "10 самых редких токенов: [('шуршат', 1), ('гремят', 1), ('петровского', 1), ('столиц', 1), ('объективную', 1), ('понравившейся', 1), ('беспрецедентно', 1), ('дооснастить', 1), ('хлебозаводская', 1), ('фрионом', 1)]\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(tokens.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f'10 самых частотных токенов: {tokens[:10]}')\n",
    "print(f'10 самых редких токенов: {tokens[-10:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ оставьте в словаре только топ 10000 самых частотных токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = dict(tokens[:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ реализуйте функцию, которая предложение переводит в вектор из чисел. То есть каждому слову из словаря сопоставляется количество раз, которое оно встретилось в предложении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4awkhecbR9om"
   },
   "outputs": [],
   "source": [
    "def text_to_bow(text: str) -> np.array:\n",
    "    \"\"\"\n",
    "    Возвращает вектор, где для каждого слова из most_common\n",
    "    указано количество его употреблений\n",
    "    input: строка\n",
    "    output: вектор размерности словаря\n",
    "    \"\"\"\n",
    "    text_tokens = Counter(text.split())\n",
    "    bow = dict(tokens)\n",
    "    for i in bow:\n",
    "        if i in text_tokens:\n",
    "            bow[i] = text_tokens[i]\n",
    "        else:\n",
    "            bow[i] = 0\n",
    "    bow = np.array(list(bow.values()), dtype='int64')\n",
    "    \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ а теперь реализуйте функцию, которая преобразует наш датасет и для каждого текста из description сопоставляет вектор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HR_D8Fn4pudv"
   },
   "outputs": [],
   "source": [
    "# #np.vectorize\n",
    "# def items_to_bow(items: np.array) -> np.array:\n",
    "#     \"\"\" Для каждого товара возвращает вектор его bow \"\"\"\n",
    "#     # Давайте для начала попробуем строить bow только из description товара\n",
    "#     description_bow = np.vectorize(text_to_bow, otypes=['object'])(items[:, 1])\n",
    "#     bow = np.stack((items[:, 0], description_bow), axis=1)\n",
    "    \n",
    "#     return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for_array\n",
    "def items_to_bow(items: np.array) -> np.array:\n",
    "    \"\"\" Для каждого товара возвращает вектор его bow \"\"\"\n",
    "    # Давайте для начала попробуем строить bow только из description товара\n",
    "    bow = np.array([text_to_bow(x[1]) for x in items])\n",
    "    \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #np.vectorize\n",
    "\n",
    "# X_train_bow = items_to_bow(X_train)\n",
    "# X_test_bow = items_to_bow(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "wwOZaEpMSQsZ",
    "outputId": "8a30c3af-3517-42bd-a5f3-36206b4b264a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 51.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#for_array\n",
    "\n",
    "X_train_bow = items_to_bow(X_train)\n",
    "X_test_bow = items_to_bow(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# чтобы видеть проход по итерациям можно использовать библиотеку tqdm\n",
    "# она работает примерно так\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJoXiCWI7VF5"
   },
   "source": [
    "### Логистическая регрессия и SVC (0.5 балла)\n",
    "\n",
    "\n",
    "Теперь описание каждого товара представлено как точка в многомерном пространстве.\n",
    "Очень важно запомнить эту идею: дальше мы будем рассматривать разные способы перехода от текста к точке в пространстве.\n",
    "\n",
    "Для BOW каждое измерение в пространстве -- какое-то слово.\n",
    "Мы предполагаем, что текст описывается набором каких-то популярных слов, которые в нём встречаются, а близкие по смыслу тексты будут использовать одинаковые слова.\n",
    "\n",
    "Обучите логистическую регрессию и SVC с базовыми параметрами.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "colab_type": "code",
    "id": "Ky3HV1rTSS9L",
    "outputId": "612a5f0d-76bd-44f4-eeeb-63b517443797"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test: 0.7046666666666667\n",
      "Wall time: 46.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr_model = LR()\n",
    "lr_model.fit(X_train_bow, y_train)\n",
    "\n",
    "print(f'Accuracy on test: {accuracy_score(lr_model.predict(X_test_bow), y_test)}')\n",
    "assert accuracy_score(lr_model.predict(X_test_bow), y_test) > 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "-c46ZT0lvF6T",
    "outputId": "4b1cb34a-201b-4dc2-9155-fdb6919c6c08"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test: 0.684\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "svc_model = LinearSVC()\n",
    "svc_model.fit(X_train_bow, y_train)\n",
    "\n",
    "print(f'Accuracy on test: {accuracy_score(svc_model.predict(X_test_bow), y_test)}')\n",
    "assert accuracy_score(svc_model.predict(X_test_bow), y_test) > 0.68"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WwKE57YZ1Hzn"
   },
   "source": [
    "### Модификация признаков (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ewMlxQezL6Ax"
   },
   "source": [
    "Добавьте title товара в bow с произвольным весом, как изменится качество?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "evqKo1r5L5BO"
   },
   "outputs": [],
   "source": [
    "#title и description включены\n",
    "def items_to_bow(items: np.array) -> np.array:\n",
    "    \"\"\" Для каждого товара возвращает вектор его bow \"\"\"\n",
    "    # Давайте для начала попробуем строить bow только из description товара\n",
    "    bow = np.array([np.array(text_to_bow(x[0])) + np.array(text_to_bow(x[1])) for x in items])\n",
    "    \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#bow из title и description\n",
    "\n",
    "X_train_bow = items_to_bow(X_train)\n",
    "X_test_bow = items_to_bow(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test: 0.7837777777777778\n",
      "Wall time: 45.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr_model = LR()\n",
    "lr_model.fit(X_train_bow, y_train)\n",
    "\n",
    "print(f'Accuracy on test: {accuracy_score(lr_model.predict(X_test_bow), y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test: 0.7536666666666667\n",
      "Wall time: 8.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "svc_model = LinearSVC()\n",
    "svc_model.fit(X_train_bow, y_train)\n",
    "\n",
    "print(f'Accuracy on test: {accuracy_score(svc_model.predict(X_test_bow), y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Db4TyqzxMnby"
   },
   "source": [
    "Нормализуйте данные (`sklearn.preprocessing.normalize`) перед обучением. Что станет с качеством и почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A8rVy6q1Mn4J"
   },
   "outputs": [],
   "source": [
    "X_train_normalized = normalize(X_train_bow)\n",
    "X_test_normalized = normalize(X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized data\n",
      "Accuracy on test: 0.6622222222222223\n",
      "Wall time: 9.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr_model = LR()\n",
    "lr_model.fit(X_train_normalized, y_train)\n",
    "\n",
    "print('Normalized data')\n",
    "print(f'Accuracy on test: {accuracy_score(lr_model.predict(X_test_normalized), y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized data\n",
      "Accuracy on test: 0.7987777777777778\n",
      "Wall time: 2.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "svc_model = LinearSVC()\n",
    "svc_model.fit(X_train_normalized, y_train)\n",
    "\n",
    "print('Normalized data')\n",
    "print(f'Accuracy on test: {accuracy_score(svc_model.predict(X_test_normalized), y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HvCAL3qGDByj"
   },
   "source": [
    "### Mystem (1 балл)\n",
    "\n",
    "Попробуйте обучиться, используя токенизатор mystem. Сравните качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hz38TqqRDY6-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n",
      "tar: Error opening archive: Failed to open 'mystem-3.0-linux3.1-64bit.tar.gz'\n",
      "\"cp\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "!wget http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
    "!tar -xvf mystem-3.0-linux3.1-64bit.tar.gz\n",
    "!cp mystem /bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60oQ-6UgDcLF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"c:\\program\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/nlpub/pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGvNHfVsDfhq"
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "mystem_analyzer = Mystem(entire_input=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mystem_item_lemmatize(item):\n",
    "    item_lemmatized = []\n",
    "    for i in item:\n",
    "        item_lemmatized.extend(mystem_analyzer.lemmatize(i))\n",
    "    item_lemmatized = ' '.join(item_lemmatized)\n",
    "    \n",
    "    return item_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\numpy\\lib\\shape_base.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[1;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[0mbuff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         \u001b[0mbuff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minarr_view\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-4e9b973eddeb>\u001b[0m in \u001b[0;36mmystem_item_lemmatize\u001b[1;34m(item)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mitem_lemmatized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mitem_lemmatized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmystem_analyzer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mitem_lemmatized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem_lemmatized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\pymystem3\\mystem.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[0mneed_encode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[0minfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lemma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\pymystem3\\mystem.py\u001b[0m in \u001b[0;36manalyze\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_analyze_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\pymystem3\\mystem.py\u001b[0m in \u001b[0;36m_analyze_impl\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_procin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_NL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m             \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_proc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_proc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[1;34m(self, input, timeout)\u001b[0m\n\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 939\u001b[1;33m                 \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    940\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m                 \u001b[1;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[1;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[0;32m   1265\u001b[0m             \u001b[1;31m# calls communicate again.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_remaining_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1268\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\threading.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1032\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m             \u001b[1;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# already determined that the C code is done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1048\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1049\u001b[0m             \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train_mystem = np.apply_along_axis(mystem_item_lemmatize, 1, X_train)\n",
    "X_test_mystem = np.apply_along_axis(mystem_item_lemmatize, 1, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymorphy2_analyzer = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pymorphy_items_lemmatize(items):\n",
    "    items_lemmatized = []\n",
    "    for item in items:\n",
    "        item_lemmatized = []\n",
    "        for i in item:\n",
    "            i = i.split()\n",
    "            for j in i:\n",
    "                ana = pymorphy2_analyzer.parse(j)\n",
    "                item_lemmatized.append(ana[0].normal_form)\n",
    "        item_lemmatized = ' '.join(item_lemmatized)\n",
    "        items_lemmatized.append(item_lemmatized)\n",
    "    items_lemmatized = np.array(items_lemmatized)\n",
    "\n",
    "    return items_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train_pymorphy = pymorphy_items_lemmatize(X_train)\n",
    "X_test_pymorphy = pymorphy_items_lemmatize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymorphy_tokens = []\n",
    "for i in X_train_pymorphy:\n",
    "    pymorphy_tokens.extend(i.split())\n",
    "pymorphy_tokens = dict(Counter(pymorphy_tokens))\n",
    "\n",
    "pymorphy_tokens = sorted(pymorphy_tokens.items(), key=lambda x: x[1], reverse=True)\n",
    "pymorphy_tokens = dict(pymorphy_tokens[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pymorphy_text_to_bow(text: str) -> np.array:\n",
    "    \"\"\"\n",
    "    Возвращает вектор, где для каждого слова из most_common\n",
    "    указано количество его употреблений\n",
    "    input: строка\n",
    "    output: вектор размерности словаря\n",
    "    \"\"\"\n",
    "    text_tokens = Counter(text.split())\n",
    "    bow = dict(pymorphy_tokens)\n",
    "    for i in bow:\n",
    "        if i in text_tokens:\n",
    "            bow[i] = text_tokens[i]\n",
    "        else:\n",
    "            bow[i] = 0\n",
    "    bow = np.array(list(bow.values()), dtype='int64')\n",
    "    \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatized_items_to_bow(items: np.array) -> np.array:\n",
    "    \"\"\" Для каждого товара возвращает вектор его bow \"\"\"\n",
    "    # Давайте для начала попробуем строить bow только из description товара\n",
    "    bow = np.array([text_to_bow(x) for x in items])\n",
    "    \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 52.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train_pymorphy_bow = lemmatized_items_to_bow(X_train_pymorphy)\n",
    "X_test_pymorphy_bow = lemmatized_items_to_bow(X_test_pymorphy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0,  2,  1, ...,  0,  0,  0],\n",
       "       [ 4,  5,  3, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [42, 15,  4, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 1,  0,  4, ...,  0,  0,  0]], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pymorphy_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pymorphy lemmatized\n",
      "Accuracy on train: 0.9335238095238095\n",
      "Accuracy on test: 0.7708888888888888\n",
      "Wall time: 48.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr_model = LR()\n",
    "lr_model.fit(X_train_pymorphy_bow, y_train)\n",
    "\n",
    "print('Pymorphy lemmatized')\n",
    "print(f'Accuracy on train: {accuracy_score(lr_model.predict(X_train_pymorphy_bow), y_train)}')\n",
    "print(f'Accuracy on test: {accuracy_score(lr_model.predict(X_test_pymorphy_bow), y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pymorphy lemmatized\n",
      "Accuracy on train: 0.9697142857142858\n",
      "Accuracy on test: 0.7402222222222222\n",
      "Wall time: 9.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "svc_model = LinearSVC()\n",
    "svc_model.fit(X_train_pymorphy_bow, y_train)\n",
    "\n",
    "print('Pymorphy lemmatized')\n",
    "print(f'Accuracy on train: {accuracy_score(svc_model.predict(X_train_pymorphy_bow), y_train)}')\n",
    "print(f'Accuracy on test: {accuracy_score(svc_model.predict(X_test_pymorphy_bow), y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pymorphy lemmatized and normalized\n",
      "Accuracy on train: 0.9085714285714286\n",
      "Accuracy on test: 0.7835555555555556\n",
      "Wall time: 5.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "svc_model = LinearSVC()\n",
    "svc_model.fit(normalize(X_train_pymorphy_bow), y_train)\n",
    "\n",
    "print('Pymorphy lemmatized and normalized')\n",
    "print(f'Accuracy on train: {accuracy_score(svc_model.predict(normalize(X_train_pymorphy_bow)), y_train)}')\n",
    "print(f'Accuracy on test: {accuracy_score(svc_model.predict(normalize(X_test_pymorphy_bow)), y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (5 баллов)\n",
    "\n",
    "Не все слова полезны одинаково, давайте попробуем [взвесить](http://tfidf.com/) их, чтобы отобрать более полезные.\n",
    "\n",
    "\n",
    "> TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "> \n",
    "> IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n",
    "\n",
    "\n",
    "В sklearn есть TfidfVectorizer, но в этом задании его использовать нельзя. Для простоты посчитайте общий tf-idf для title и description (то есть каждому объекту надо сопоставить вектор, где как документ будет рассматриваться конкатенация title и description)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ составьте словарь, где каждому слову из изначального словаря будет стоять в соответствии количество документов, где это слово встретилось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_tokens = dict(pymorphy_tokens)\n",
    "for token in tfidf_tokens:\n",
    "    tfidf_tokens[token] = 0\n",
    "\n",
    "for i in X_train_pymorphy:\n",
    "    i = set(i.split())\n",
    "    for j in i:\n",
    "        if j in tfidf_tokens:\n",
    "            tfidf_tokens[j] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ реализуйте функцию, где тексту в соответствие ставится tf-idf вектор. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6i5zFpD9rbtz"
   },
   "outputs": [],
   "source": [
    "def text_to_tfidf(text: str) -> np.array:\n",
    "    \"\"\"\n",
    "    Возвращает вектор, где для каждого слова из словаря\n",
    "    указан tf-idf\n",
    "    \"\"\"\n",
    "    text = text.split()\n",
    "    text_terms_count = Counter(text)\n",
    "    tfidf = [\n",
    "        text_terms_count[i] / len(text) *\n",
    "        log(len(X_train_pymorphy) / tfidf_tokens[i])\n",
    "        for i in tfidf_tokens\n",
    "    ]\n",
    "    \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ а теперь реализуйте функцию, которая преобразует наш датасет и для каждого объекта сопоставляет вектор tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def items_to_tfidf(items: np.array) -> np.array:\n",
    "    \"\"\" \n",
    "    Для каждого товара возвращает его tfidf вектор\n",
    "    \"\"\"\n",
    "    tfidf = np.array([text_to_tfidf(x) for x in items])\n",
    "    \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train_tfidf = items_to_tfidf(X_train_pymorphy)\n",
    "X_test_tfidf = items_to_tfidf(X_test_pymorphy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21000, 10000)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-YFA-8kE1RHk"
   },
   "source": [
    "__Задание:__ обучите логистическую регрессию и SVC, оцените качество (accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ULrXsF1m5sU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF\n",
      "Accuracy on train: 0.9398571428571428\n",
      "Accuracy on test: 0.7944444444444444\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr_model = LR(C=10)\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('TF-IDF')\n",
    "print(f'Accuracy on train: {accuracy_score(lr_model.predict(X_train_tfidf), y_train)}')\n",
    "print(f'Accuracy on test: {accuracy_score(lr_model.predict(X_test_tfidf), y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF\n",
      "Accuracy on train: 0.9601428571428572\n",
      "Accuracy on test: 0.8131111111111111\n",
      "Wall time: 4.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "svc_model = LinearSVC(C=1)\n",
    "svc_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('TF-IDF')\n",
    "print(f'Accuracy on train: {accuracy_score(svc_model.predict(X_train_tfidf), y_train)}')\n",
    "print(f'Accuracy on test: {accuracy_score(svc_model.predict(X_test_tfidf), y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jFdy3lUFDsOr"
   },
   "source": [
    "### Hashing Vectorizer (1 балл)\n",
    "\n",
    "Попробуйте использовать `sklearn.feature_extraction.text.HashingVectorizer` для векторизации текстов.\n",
    "Обязательно оцените качество работы алгоритмов классификации с использованием новой векторизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y666HTrqDq1m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "hashing_vec = HashingVectorizer()\n",
    "X_train_hashing = hashing_vec.fit_transform(X_train_pymorphy)\n",
    "X_test_hashing = hashing_vec.fit_transform(X_test_pymorphy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashing Vectorizer\n",
      "Accuracy on train: 0.975047619047619\n",
      "Accuracy on test: 0.8144444444444444\n",
      "Wall time: 35.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr_model = LR(C=10)\n",
    "lr_model.fit(X_train_hashing, y_train)\n",
    "\n",
    "print('Hashing Vectorizer')\n",
    "print(f'Accuracy on train: {accuracy_score(lr_model.predict(X_train_hashing), y_train)}')\n",
    "print(f'Accuracy on test: {accuracy_score(lr_model.predict(X_test_hashing), y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashing Vectorizer\n",
      "Accuracy on train: 0.9797619047619047\n",
      "Accuracy on test: 0.8291111111111111\n",
      "Wall time: 2.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "svc_model = LinearSVC(C=1)\n",
    "svc_model.fit(X_train_hashing, y_train)\n",
    "\n",
    "print('Hashing Vectorizer')\n",
    "print(f'Accuracy on train: {accuracy_score(svc_model.predict(X_train_hashing), y_train)}')\n",
    "print(f'Accuracy on test: {accuracy_score(svc_model.predict(X_test_hashing), y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_hashing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-b1e9ea3992d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train_hashing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_hashing' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vQZ61xSsTpZI"
   },
   "source": [
    "### Word Vectors (3 балла)\n",
    "\n",
    "Давайте попробуем другой подход -- кажому слову сопоставим какой-то эмбеддинг (вектор).\n",
    "\n",
    "Вектора будут небольшой размерности. Таким образом мы снизим количество параметров в модели.\n",
    "\n",
    "Вектора мы возьмём уже готовые (обученные на текстах их интернета), так что наша модель будет знать некоторую дополнительную информацию о внешнем мире."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "colab_type": "code",
    "id": "T38J27NcYGx5",
    "outputId": "57fa3a9f-13a3-4fa1-d13c-3c0c49a86a71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/0x7oxso6x93efzj/ru.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zfse4xVbgMIr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tar: Error opening archive: Failed to open 'ru.tar.gz'\n"
     ]
    }
   ],
   "source": [
    "!tar -xzf ru.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sy2TXmQ2jZSY"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "model = FastText.load_fasttext_format('ru.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[ 0.02916384  0.02167605  0.05127367 -0.00971958  0.0465235  -0.03945766\n",
      "  0.02737866  0.00638128 -0.03774629 -0.04257201 -0.00995653  0.02291315\n",
      " -0.02301722  0.06697998 -0.03674482 -0.02403202 -0.05404469  0.01372932\n",
      "  0.00926399 -0.0013149   0.11941359 -0.022448    0.04011497  0.06980549\n",
      "  0.00407011 -0.09384539  0.03050164 -0.02578281 -0.03525181 -0.06603175\n",
      "  0.04752798  0.05874675  0.01983666  0.06092105 -0.00957561  0.08307806\n",
      " -0.01288903  0.04705157  0.02198839 -0.00649013 -0.0171444   0.03302203\n",
      "  0.02124882 -0.01902875 -0.05235172  0.03458685 -0.01409259 -0.07477519\n",
      "  0.01916078  0.02985001  0.0086322   0.03051201  0.02831862  0.04549561\n",
      "  0.00761138 -0.05459622  0.09056009 -0.08807947 -0.05420396 -0.04793203\n",
      " -0.05672329 -0.03025264 -0.03024072 -0.05890108 -0.03137474  0.03292617\n",
      "  0.05440779 -0.04548327 -0.07266086 -0.09327219  0.07247883  0.0111061\n",
      "  0.01824225 -0.10570452  0.05110046 -0.04659343 -0.03277056 -0.00803401\n",
      " -0.03978698  0.00826598 -0.01074128  0.018431   -0.10150263 -0.00472604\n",
      "  0.06706332  0.02466901  0.09045192 -0.05226929  0.04866098 -0.02843297\n",
      "  0.04756537  0.00261342  0.06845197  0.00082511 -0.00547984  0.0100649\n",
      "  0.02135489 -0.01437242  0.00191435  0.11989547  0.02357679  0.07061605\n",
      "  0.03375214  0.05462346  0.08270866  0.00126649  0.03054527  0.04314573\n",
      " -0.00719835 -0.02799017  0.00249404  0.00139046 -0.04099929  0.00526204\n",
      "  0.01386764  0.02106066  0.00887202  0.05943111 -0.07185322  0.03263306\n",
      "  0.00284878  0.03816929  0.0210096  -0.030828    0.00502779  0.09250114\n",
      "  0.02399154  0.05744717 -0.04319151  0.04075926 -0.03877947  0.0605263\n",
      " -0.00837917 -0.04922852 -0.04570796  0.02973622 -0.01798053  0.00413011\n",
      " -0.00712464 -0.01312802  0.05847022 -0.07881333 -0.02204878  0.03086594\n",
      "  0.02965177 -0.0073295  -0.02443145 -0.06222062  0.01083152  0.06009534\n",
      " -0.02042049  0.06301811  0.02287635 -0.03021961  0.04831248  0.02882019\n",
      "  0.04446645 -0.01677353 -0.08272323 -0.06830658  0.08947854  0.03370909\n",
      " -0.00895046 -0.00681254 -0.02059644 -0.09527113  0.02611189 -0.06112244\n",
      "  0.01080315  0.01901113  0.00810233  0.00742132  0.10493557 -0.00522375\n",
      "  0.05826566  0.03236291  0.03787734 -0.05026894 -0.08401242  0.02860721\n",
      " -0.05106218  0.02631241  0.02631763  0.06924202  0.03319636  0.00980412\n",
      "  0.04016861  0.03428936  0.00652957 -0.01058654 -0.0245588   0.1464914\n",
      " -0.01041028  0.03553488 -0.07482928 -0.01063148 -0.0342233  -0.01662586\n",
      " -0.00029508  0.04694034 -0.00062491 -0.0435293  -0.01315623  0.07061336\n",
      "  0.01603698  0.02374655  0.05453315  0.00253603 -0.0313729  -0.02740866\n",
      "  0.04278845 -0.00810288  0.03973977  0.07674816  0.04658518 -0.02685211\n",
      " -0.05009724  0.0060723  -0.04231661  0.02584185 -0.03419575 -0.03799306\n",
      "  0.06701688 -0.1245426   0.03846397 -0.0855662  -0.01193651  0.04968415\n",
      "  0.03559558  0.10029506  0.05714916  0.01145345 -0.03564315 -0.00924199\n",
      "  0.08630151  0.08049053  0.05822275 -0.05224873 -0.02462301  0.05832206\n",
      " -0.04124978  0.00186134  0.00782246  0.01179015 -0.02291097  0.00614069\n",
      "  0.01782681  0.02190027  0.04341367  0.06151633 -0.01183114 -0.00141502\n",
      "  0.06193598  0.0611085  -0.02373199 -0.05797793 -0.02269631  0.11511736\n",
      " -0.04581353 -0.05082048 -0.04706197  0.0429772   0.00409648 -0.0141248\n",
      "  0.01417164  0.00575812 -0.07616108 -0.01051838  0.05149659  0.02367133\n",
      "  0.00073724  0.05957585 -0.11871962  0.03876314  0.03472188 -0.02344368\n",
      " -0.01165281 -0.01397923  0.08815268  0.03459521  0.07113555 -0.03984846\n",
      " -0.01600395  0.01932258  0.01351069 -0.06409036 -0.02024848  0.05895981\n",
      "  0.02591374 -0.04027611  0.00654722  0.05093394 -0.02461737  0.02561689\n",
      " -0.01412898 -0.00366109 -0.06719207  0.00742674 -0.02095614 -0.06263787]\n"
     ]
    }
   ],
   "source": [
    "# как мы видим, каждому словву данная модель сопоставляет вектор размерности 300\n",
    "\n",
    "print(model['привет'].shape)\n",
    "print(model['привет'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H49QR_jhjmCa"
   },
   "outputs": [],
   "source": [
    "# Будем рассматривать эмбеддинг предложения как сумму эмбеддингов токенов\n",
    "\n",
    "def sentence_embedding(sentence: str) -> np.array:\n",
    "    \"\"\"\n",
    "    Складывает вектора токенов строки sentence\n",
    "    \"\"\"\n",
    "    sentence = sentence.split()\n",
    "    embedding = np.zeros(300)\n",
    "    for word in sentence:\n",
    "        if word in model:\n",
    "            embedding += model[word]\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gj6U_hjtlllV"
   },
   "outputs": [],
   "source": [
    "assert np.allclose(sentence_embedding('сдаётся уютный , тёплый гараж для стартапов в ml')[::50],\n",
    "                   np.array([ 0.08189847,  0.07249198, -0.15601222,  0.03782297,  0.09215296, -0.23092946]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ сделайте все то же, что в предыдущих пунктах -- реализуйте функцию, которая преобразует данные, а затем обучите логистическую регрессию и SVM, оцените качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8tfhc-PFmGvu",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def items_to_embeddings(items):\n",
    "    embeddings = np.array([sentence_embedding(x) for x in items])\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train_embeddings = items_to_embeddings(X_train_pymorphy)\n",
    "X_test_embeddings = items_to_embeddings(X_test_pymorphy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21000, 300)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings\n",
      "Accuracy on train: 0.7106666666666667\n",
      "Accuracy on test: 0.6103333333333333\n",
      "Wall time: 4min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr_model = LR(C=10, max_iter=200)\n",
    "lr_model.fit(X_train_embeddings, y_train)\n",
    "\n",
    "print('Embeddings')\n",
    "print(f'Accuracy on train: {accuracy_score(lr_model.predict(X_train_embeddings), y_train)}')\n",
    "print(f'Accuracy on test: {accuracy_score(lr_model.predict(X_test_embeddings), y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings\n",
      "Accuracy on train: 0.7037142857142857\n",
      "Accuracy on test: 0.6093333333333333\n",
      "Wall time: 9min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "svc_model = LinearSVC(C=1, max_iter=5000)\n",
    "svc_model.fit(X_train_embeddings, y_train)\n",
    "\n",
    "print('Embeddings')\n",
    "print(f'Accuracy on train: {accuracy_score(svc_model.predict(X_train_embeddings), y_train)}')\n",
    "print(f'Accuracy on test: {accuracy_score(svc_model.predict(X_test_embeddings), y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cVEdlFostSnX"
   },
   "source": [
    "### Что дальше? (6 баллов)\n",
    "\n",
    "Для получения максимальной оценки вам нужно решить любые 2 пункта. Решение каждого пункта даст вам полтора балла:\n",
    "\n",
    "1. Реализовать N-Gram модели текстовой классификации (__1.5 балла__)\n",
    "\n",
    "2. Поработать с другими эмбеддингами для слов (например word2vec или GloVe) (__1.5 балла__)\n",
    "\n",
    "3. Другие способы токенизации (pymorphy2, spaCy) (__1.5 балла__)\n",
    "\n",
    "4. Добиться качества > 0.765 на тестовых данных (попробуйте другие токенизаторы, предобработку текста, и любые другие идеи, которые вам придут в голову) (__1.5 балла__)\n",
    "\n",
    "Снабжайте код пояснениями и графиками.\n",
    "Обязательно необходимо написать вывод по каждому пункту, который вы реализуете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# две строчки кода здесь"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
